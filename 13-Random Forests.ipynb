## Decision Tree

#### Problem Statement: 1- Company_data
#### A cloth manufacturing company is interested to know about the segment or attributes causes high sale. Approach - A decision tree can be built with target variable Sale (we will first convert it in categorical variable) & all other variable will be independent in the analysis.  


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import classification_report
from sklearn import preprocessing
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import BaggingClassifier


data = pd.read_csv('Company_Data.csv')
data

data.head()

data.describe()

val=[]

for value in data["Sales"]:
    if value <= 10:
        val.append('0')
        
    else:
        val.append('1')
            
data["Sales"]= val



data

data['Sales'].describe

data['Sales'].value_counts()

data1= pd.get_dummies(data, columns=['ShelveLoc', 'Urban', 'US'])
data1

label_encoder = preprocessing.LabelEncoder()
data1['Sales']= label_encoder.fit_transform(data1['Sales'])
# encoding the Sales data with respect to entire dataset

X = data1.iloc[:, 1:14]
y = data1['Sales']
seed = 7
kfold = KFold (n_splits=10)
cart = DecisionTreeClassifier()
num_trees= 100
model = BaggingClassifier (base_estimator = cart, n_estimators = num_trees, random_state = seed)
results =cross_val_score(model, X, y, cv = kfold )
print(results.mean())

### We can see that there the bagging technique can help to provide accuracy of 86%. Lets train test the model to check the accuracy

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=30)

model = DecisionTreeClassifier(criterion = 'entropy', max_depth=3)
model.fit(x_train,y_train)

pred_train = model.predict(x_train)

pred_train

results

preds = model.predict(x_test)
pd.Series(preds).value_counts()

y_test

preds

pd.crosstab(y_test, preds)

np.mean(preds==y_test)

### It can be observed an accuracy of 86% and can be considered as a good model

# random forest classification
from pandas import read_csv
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

X = data1.iloc[:, 1:14]
y = data1['Sales']
num_trees = 100
max_features = 5
kfold = KFold(n_splits=10)
model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)
results = cross_val_score(model, X, y, cv=kfold)
print(results.mean()) 


## The random forest classification also gives an accuracy of over 86%

# adaboost classifier
from pandas import read_csv
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import AdaBoostClassifier

X = data1.iloc[:, 1:14]
y = data1['Sales']

num_trees = 10
seed=7
kfold = KFold(n_splits=10)
model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)
results = cross_val_score(model, X, y, cv=kfold)
print(results.mean())


### AdaBoost provides an accuracy of 87%

### Stacking Ensemble for Classifications

# Stacking Ensemble for Classifications

from pandas import read_csv
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier


X = data1.iloc[:, 1:14]
y = data1['Sales']
kfold = KFold(n_splits=10)

# create the sub models
estimators = []
model1 = LogisticRegression(max_iter=500)
estimators.append(('logistic', model1))
model2 = DecisionTreeClassifier()
estimators.append(('cart', model2))
model3 = SVC()
estimators.append(('svm', model3))

# create the ensemble model
ensemble = VotingClassifier(estimators)
results = cross_val_score(ensemble, X, y, cv=kfold)
print(results.mean())

### This model gives the highest accuracy of 88% 

estimators

## Problem 2: Use decision trees to prepare a model on fraud data treating those who have taxable_income <= 30000 as "Risky" and others are "Good"

chk = pd.read_csv('Fraud_check.csv')
chk

chk1= pd.get_dummies(chk, columns=['Undergrad', 'Marital.Status', 'Urban'])
chk1

trt=[]

for value in chk1["Taxable.Income"]:
    if value <= 30000:
        trt.append('risky')
        
    else:
        trt.append('good')
            
chk1["Taxable.Income"]= trt

label_encoder = preprocessing.LabelEncoder()
chk1['Taxable.Income']= label_encoder.fit_transform(chk1['Taxable.Income']) 

chk1

chk1['Taxable.Income'].value_counts()

chk1.iloc[:, 1:10]

X = chk1.iloc[:, 1:10]
y = chk1.iloc[:, :1]

y.value_counts()

chk1.columns

# Bagging Decision Tree Classification

X = chk1.iloc[:, 1:10]
y = chk1.iloc[:, :1]

kfold = KFold(n_splits=10)
cart = DecisionTreeClassifier()
num_trees = 100
model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees)
results = cross_val_score(model, X, y, cv=kfold)
print(results.mean())

### The above model gives an accuracy of 73% and above

### Using the Random forest Classification to test the accuracy

# Random Forest Classification


X = chk1.iloc[:, 1:10]
y = chk1.iloc[:, :1]
num_trees = 100
max_features = 3
kfold = KFold(n_splits=10)
model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)
results = cross_val_score(model, X, y, cv=kfold)
print(results.mean())

### Random forest provides an accuracy of 72% and hence other models must be trained and tested

# AdaBoost Classification


X = chk1.iloc[:, 1:10]
y = chk1.iloc[:, :1]

num_trees = 10
kfold = KFold(n_splits=10)
model = AdaBoostClassifier(n_estimators=num_trees)
results = cross_val_score(model, X, y, cv=kfold)
print(results.mean())


### AdaBoost gives an accuracy of 79% and above and can be considered as a good model

# Stacking Ensemble for Classification


X = chk1.iloc[:, 1:10]
y = chk1.iloc[:, :1]
kfold = KFold(n_splits=10)

# create the sub models
estimators = []
model1 = LogisticRegression(max_iter=500)
estimators.append(('logistic', model1))
model2 = DecisionTreeClassifier()
estimators.append(('cart', model2))
model3 = SVC()
estimators.append(('svm', model3))

# create the ensemble model
ensemble = VotingClassifier(estimators)
results = cross_val_score(ensemble, X, y, cv=kfold)
print(results.mean())

### Stacking ensemble gives an accuracy of 79% and hence can be considered as an equally good model as AdaBoost is


